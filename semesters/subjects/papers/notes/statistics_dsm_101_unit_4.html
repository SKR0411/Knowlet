<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="description" content="Comprehensive notes for DSM-101: Unit 4 - Regression and Curve Fitting. Covers key topics such as 1. Regression: Types of Regression (Lines), 2. Regression Coefficients and their Properties, 3. Angle Between Two Regression Lines, 4. Principle of Least Squares, 5. Fitting of Linear, Polynomials, and Exponential Curves, 6. Coefficient of Determination (r²). Includes definitions, explanations, and short summaries for college students.">
    <meta name="keywords" content="and, angle, between, coefficient, coefficients, curve, curves, determination, dsm101, exponential, fitting, least, linear, lines, polynomials, principle, properties, regression, squares, their, two, types, unit">

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DSM-101: Unit 4 - Regression and Curve Fitting</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            background-color: #f4f4f9;
            color: #333;
            margin: 0;
            padding: 20px;
        }
        .container {
            max-width: 900px;
            margin: auto;
            background: #fff;
            padding: 20px 30px;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #8e44ad;
            padding-bottom: 10px;
            text-align: center;
        }
        h2 {
            color: #8e44ad;
            border-bottom: 2px solid #E8DAEF;
            padding-bottom: 8px;
            margin-top: 30px;
        }
        h3 {
            color: #7d3c98;
            margin-top: 25px;
        }
        p {
            margin-bottom: 15px;
        }
        ul, ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }
        li {
            margin-bottom: 8px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #8e44ad;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        tr:hover {
            background-color: #f1f1f1;
        }
        blockquote {
            background: #E8DAEF;
            border-left: 5px solid #8e44ad;
            margin: 20px 0;
            padding: 15px 20px;
            font-style: italic;
            color: #555;
        }
        .exam-tip, .note {
            background: #fffbe6;
            border: 1px solid #ffe58f;
            border-left-width: 5px;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .exam-tip strong {
            color: #d9534f;
        }
        .note strong {
            color: #0275d8;
        }
        .formula {
            background: #f4ecf7;
            border: 1px solid #d7bde2;
            padding: 15px;
            margin: 20px 0;
            font-family: "Courier New", Courier, monospace;
            font-size: 1.1em;
            overflow-x: auto;
        }
        #toc {
            background: #E8DAEF;
            border: 1px solid #8e44ad;
            padding: 15px 25px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        #toc h2 {
            margin-top: 0;
            border-bottom: none;
            color: #2c3e50;
        }
        #toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        #toc ul li a {
            text-decoration: none;
            color: #7d3c98;
        }
        #toc ul li a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Unit 4: Regression and Curve Fitting</h1>

        <div id="toc">
            <h2>Table of Contents</h2>
            <ul>
                <li><a href="#regression-lines">1. Regression: Types of Regression (Lines)</a></li>
                <li><a href="#regression-coeffs">2. Regression Coefficients and their Properties</a></li>
                <li><a href="#angle">3. Angle Between Two Regression Lines</a></li>
                <li><a href="#least-squares">4. Principle of Least Squares</a></li>
                <li><a href="#fitting">5. Fitting of Linear, Polynomials, and Exponential Curves</a></li>
                <li><a href="#determination">6. Coefficient of Determination (r²)</a></li>
            </ul>
        </div>

        <section id="regression-lines">
            <h2>1. Regression: Types of Regression (Lines)</h2>
            <p>If correlation shows a relationship exists, regression describes that relationship with an equation. This equation can be used for <strong>prediction</strong>.</p>
            <p>A "line of regression" is the <strong>line of best fit</strong> for the data. In bivariate analysis, there are <strong>two</strong> main types (lines) of linear regression.</p>

            <h3>1. Regression Line of Y on X</h3>
            <p>This line is used to <strong>predict the value of Y, given a value of X</strong>.</p>
            <p><strong>Equation:</strong> (Y - ȳ) = b<sub>yx</sub> * (X - x̄)</p>
            <p>Here, <strong>b<sub>yx</sub></strong> is the <strong>regression coefficient of Y on X</strong> (the slope). It represents the average change in Y for a one-unit change in X.</p>

            <h3>2. Regression Line of X on Y</h3>
            <p>This line is used to <strong>predict the value of X, given a value of Y</strong>.</p>
            <p><strong>Equation:</strong> (X - x̄) = b<sub>xy</sub> * (Y - ȳ)</p>
            <p>Here, <strong>b<sub>xy</sub></strong> is the <strong>regression coefficient of X on Y</strong>. It represents the average change in X for a one-unit change in Y.</p>

            <div class="note">
                <strong>Note:</strong> Both regression lines always pass through the point (x̄, ȳ), which is the mean of x and the mean of y.
            </div>
        </section>

        <hr>

        <section id="regression-coeffs">
            <h2>2. Regression Coefficients and their Properties</h2>
            <p>The coefficients b<sub>yx</sub> and b<sub>xy</sub> are the slopes of the two regression lines.</p>

            <h3>Formulas for Coefficients:</h3>
            <div class="formula">
                b<sub>yx</sub> = Cov(x, y) / σ<sub>x</sub>² = r * (σ<sub>y</sub> / σ<sub>x</sub>)
                <br>
                b<sub>xy</sub> = Cov(x, y) / σ<sub>y</sub>² = r * (σ<sub>x</sub> / σ<sub>y</sub>)
            </div>
            <div class="formula">
                <strong>Computational Formulas:</strong>
                <br>
                b<sub>yx</sub> = [ n(Σxy) - (Σx)(Σy) ] / [ n(Σx²) - (Σx)² ]
                <br>
                b<sub>xy</sub> = [ n(Σxy) - (Σx)(Σy) ] / [ n(Σy²) - (Σy)² ]
            </div>

            <h3>Properties of Regression Coefficients:</h3>
            <ol>
                <li>
                    <strong>Geometric Mean:</strong> The correlation coefficient 'r' is the geometric mean of the two regression coefficients.
                    <div class="formula">r² = b<sub>yx</sub> * b<sub>xy</sub>   =>   r = ± sqrt(b<sub>yx</sub> * b<sub>xy</sub>)</div>
                </li>
                <li>
                    <strong>Sign:</strong> 'r', b<sub>yx</sub>, and b<sub>xy</sub> all have the <strong>same sign</strong>.
                </li>
                <li>
                    <strong>Magnitude:</strong> If one regression coefficient is greater than 1, the other *must* be less than 1 (as their product, r², cannot exceed 1).
                </li>
            </ol>
            <div class="exam-tip">
                <strong>Exam Tip:</strong> A classic question: "The two regression coefficients are 1.6 and 0.9. Is this possible?"
                <br>
                - Answer: No. r² = 1.6 * 0.9 = 1.44, which is > 1. This is impossible.
            </div>
        </section>

        <hr>

        <section id="angle">
            <h2>3. Angle Between Two Regression Lines</h2>
            <p>The two regression lines intersect at (x̄, ȳ). The angle (θ) between them indicates the strength of the correlation.</p>
            <div class="formula">
                tan(θ) = [ (1 - r²) / (r) ] * [ (σ<sub>x</sub> * σ<sub>y</sub>) / (σ<sub>x</sub>² + σ<sub>y</sub>²) ]
            </div>
            
            <h3>Key Insights:</h3>
            <ul>
                <li>If <strong>r = 0</strong>: tan(θ) = ∞, so <strong>θ = 90°</strong>. The lines are perpendicular. The variables are uncorrelated.</li>
                <li>If <strong>r = +1 or -1</strong>: tan(θ) = 0, so <strong>θ = 0°</strong>. The two lines are <strong>coincident</strong> (they become the same line). This means perfect correlation.</li>
            </ul>
        </section>

        <hr>

        <section id="least-squares">
            <h2>4. Principle of Least Squares</h2>
            <p>This is the fundamental method used to find the "best-fit" line (the regression line) for a set of data points.</p>
            
            <blockquote>
                <strong>Principle:</strong> The line of best fit is the one that <strong>minimizes the sum of the squares of the vertical errors (residuals)</strong>.
            </blockquote>
            
            <ul>
                <li><strong>Residual (Error):</strong> e<sub>i</sub> = (Observed y<sub>i</sub>) - (Predicted ŷ<sub>i</sub>)</li>
                <li><strong>Goal:</strong> Minimize the Sum of Squared Errors (SSE).</li>
            </ul>
            <div class="formula">
                Minimize: SSE = Σ (e<sub>i</sub>)² = Σ (y<sub>i</sub> - ŷ<sub>i</sub>)²
            </div>
            <p>For a straight line ŷ = a + bx, we use calculus (partial derivatives w.r.t. 'a' and 'b') to find the values that minimize SSE. This process generates the "Normal Equations."</p>
            </section>

        <hr>

        <section id="fitting">
            <h2>5. Fitting of Linear, Polynomials, and Exponential Curves</h2>
            <p>Using the Principle of Least Squares, we can derive the <strong>Normal Equations</strong> needed to fit specific curves to data.</p>
            
            <h3>1. Fitting a Linear Equation (Straight Line)</h3>
            <p><strong>Equation:</strong> y = a + bx</p>
            <blockquote>
                <strong>Normal Equations:</strong>
                <ol>
                    <li>Σy = n*a + b*(Σx)</li>
                    <li>Σxy = a*(Σx) + b*(Σx²)</li>
                </ol>
            </blockquote>
            <p>Solve these two simultaneous equations for 'a' and 'b'.</p>

            <h3>2. Fitting a Polynomial (Parabola / Quadratic)</h3>
            <p><strong>Equation:</strong> y = a + bx + cx²</p>
            <blockquote>
                <strong>Normal Equations:</strong>
                <ol>
                    <li>Σy = n*a + b*(Σx) + c*(Σx²)</li>
                    <li>Σxy = a*(Σx) + b*(Σx²) + c*(Σx³)</li>
                    <li>Σx²y = a*(Σx²) + b*(Σx³) + c*(Σx⁴)</li>
                </ol>
            </blockquote>
            <p>Solve these three simultaneous equations for 'a', 'b', and 'c'.</p>

            <h3>3. Fitting an Exponential Curve</h3>
            <p><strong>Equation:</strong> y = a * b<sup>x</sup></p>
            <p>This is not linear. We must <strong>transform it</strong> by taking the logarithm.</p>
            <div class="formula">
                log(y) = log(a) + x * log(b)
            </div>
            <p>Now, let <strong>Y = log(y)</strong>, <strong>A = log(a)</strong>, and <strong>B = log(b)</strong>.
            <br>The equation becomes a straight line: <strong>Y = A + Bx</strong></p>
            <p>We use the normal equations for a straight line, but with Y instead of y:</p>
            <blockquote>
                <strong>Normal Equations (Exponential):</strong>
                <ol>
                    <li>ΣY = n*A + B*(Σx)   =>   Σ(log y) = n*log(a) + log(b)*(Σx)</li>
                    <li>ΣxY = A*(Σx) + B*(Σx²)   =>   Σ(x log y) = log(a)*(Σx) + log(b)*(Σx²)</li>
                </ol>
            </blockquote>
            <p>Solve for A and B, then find <strong>a = antilog(A)</strong> and <strong>b = antilog(B)</strong>.</p>
        </section>

        <hr>

        <section id="determination">
            <h2>6. Coefficient of Determination (r²)</h2>
            <blockquote>
                <strong>Coefficient of Determination (r²):</strong> The square of the correlation coefficient (r). It represents the <strong>proportion of the total variance in the dependent variable (Y) that is explained or accounted for by the linear relationship with the independent variable (X)</strong>.
            </blockquote>
            <ul>
                <li><strong>Range:</strong> 0 ≤ r² ≤ 1 (since it's a square).</li>
                <li><em>Example:</em> If r = 0.9, then r² = 0.81.</li>
                <li><strong>Interpretation:</strong> This means <strong>81%</strong> of the variation in Y can be explained by X. The remaining 19% (1 - r²) is unexplained variation, due to other factors or random error.</li>
            </ul>
        </section>

    </div>
</body>
</html>